{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec07a7fc-daa4-4e91-9c25-12d3416b7e85",
   "metadata": {},
   "source": [
    "# **Extracción de datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fbdf8-3b56-4cc1-a9c9-56e36aa70eb0",
   "metadata": {},
   "source": [
    "## API NEWSAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ccf5df-af0f-48ed-b8fc-b5932e514f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías instaladas exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Lista de librerías que deseas instalar\n",
    "# En este caso, las librerías son 'schedule' para la programación de tareas y 'python-dateutil' para el manejo de fechas\n",
    "libraries = ['schedule', 'python-dateutil']\n",
    "\n",
    "# Función para instalar las librerías\n",
    "def install_libraries():\n",
    "    \"\"\"\n",
    "    Esta función recorre la lista de librerías especificadas y las instala utilizando pip.\n",
    "    Utiliza el ejecutable de Python en el entorno actual para asegurarse de que las librerías\n",
    "    se instalen correctamente en el entorno activo.\n",
    "    \"\"\"\n",
    "    for library in libraries:\n",
    "        try:\n",
    "            # Ejecuta el comando pip install para cada librería en la lista\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", library])\n",
    "            print(f\"Librería '{library}' instalada exitosamente.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            # En caso de error durante la instalación de alguna librería, se captura y muestra un mensaje\n",
    "            print(f\"Hubo un error al intentar instalar '{library}': {e}\")\n",
    "\n",
    "# Llamar a la función para instalar las librerías\n",
    "install_libraries()\n",
    "\n",
    "# Mensaje final indicando que el proceso de instalación ha terminado\n",
    "print(\"Proceso de instalación de librerías finalizado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23e41f7-095b-4ebc-b768-fe5e21c99ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la extracción de noticias...\n",
      "Realizando solicitud a la API con los parámetros: {'q': 'technology', 'sortBy': 'publishedAt', 'apiKey': 'e3abbf3202924217ae7b0beee1f689d4'}\n",
      "Solicitud a la API exitosa.\n",
      "Se han encontrado 100 artículos.\n",
      "Artículos filtrados y convertidos a DataFrame:\n",
      "                                              Título  \\\n",
      "0  Apple CEO Tim Cook Declares New iPhone Era In ...   \n",
      "1  “Placebo AI”: Is Your Business Using Automatio...   \n",
      "2              Apple Forced To Cancel Beloved iPhone   \n",
      "3  Chinese automobiles gaining popularity among G...   \n",
      "4  Samsung and Apple's plans for thinner smartpho...   \n",
      "\n",
      "                                         Descripción  \\\n",
      "0  The Apple CEO’s latest visit to London saw him...   \n",
      "1  Picture the last time you called a service hot...   \n",
      "2  Apple is removing the current iPhone SE from s...   \n",
      "3  ACCRA, Dec. 14 (Xinhua) -- Ghanaians are incre...   \n",
      "4  Samsung and Apple's plans for thinner smartpho...   \n",
      "\n",
      "                                               Autor  \\\n",
      "0  David Phelan, Senior Contributor, \\n David Phe...   \n",
      "1  Cornelia C. Walther, Contributor, \\n Cornelia ...   \n",
      "2  Ewan Spence, Senior Contributor, \\n Ewan Spenc...   \n",
      "3                                            Justice   \n",
      "4                                Ian Carlos Campbell   \n",
      "\n",
      "                                                 URL  Fecha de publicación  \\\n",
      "0  https://www.forbes.com/sites/davidphelan/2024/...  2024-12-14T17:25:39Z   \n",
      "1  https://www.forbes.com/sites/corneliawalther/2...  2024-12-14T17:24:42Z   \n",
      "2  https://www.forbes.com/sites/ewanspence/2024/1...  2024-12-14T17:21:07Z   \n",
      "3  https://www.thestar.com.my/news/world/2024/12/...  2024-12-14T17:14:00Z   \n",
      "4  https://www.androidpolice.com/samsung-apple-th...  2024-12-14T17:11:42Z   \n",
      "\n",
      "                                              Imagen  \n",
      "0  https://imageio.forbes.com/specials-images/ima...  \n",
      "1  https://imageio.forbes.com/specials-images/ima...  \n",
      "2  https://imageio.forbes.com/specials-images/ima...  \n",
      "3  https://cdn.thestar.com.my/Themes/img/newTsol_...  \n",
      "4  https://static1.anpoimages.com/wordpress/wp-co...  \n",
      "Datos guardados en \"articulos_filtrados.csv\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Programar la ejecución cada 12 horas\\nschedule.every(12).hours.do(fetch_news)\\n\\n# Mantener el script en ejecución\\nwhile True:\\n    schedule.run_pending()\\n    time.sleep(1)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests  # Biblioteca para realizar solicitudes HTTP a servidores web.\n",
    "import pandas as pd  # Biblioteca para manipulación y análisis de datos en formato tabular (DataFrame).\n",
    "from datetime import datetime  # Módulo para trabajar con fechas y horas en Python.\n",
    "from dateutil import parser  # Biblioteca para analizar cadenas de texto con fechas y convertirlas a objetos datetime.\n",
    "import schedule  # Biblioteca para programar y ejecutar tareas periódicas a intervalos regulares.\n",
    "import time  # Módulo que permite trabajar con el tiempo, incluyendo pausas en la ejecución del programa.\n",
    "\n",
    "# Función para obtener noticias\n",
    "def fetch_news():\n",
    "    print(\"Iniciando la extracción de noticias...\")  # Agregado para saber si está comenzando\n",
    "\n",
    "    # Definir la URL de la API con parámetros\n",
    "    url = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "    # Establecer los parámetros de la solicitud\n",
    "    params = {\n",
    "        'q': 'technology',  # Solo noticias sobre tecnología\n",
    "        'sortBy': 'publishedAt',  # Ordenar por fecha de publicación\n",
    "        'apiKey': 'e3abbf3202924217ae7b0beee1f689d4'  # Tu API Key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(f\"Realizando solicitud a la API con los parámetros: {params}\")  # Muestra los parámetros de la solicitud\n",
    "        # Realizar la solicitud a la API\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Verificar si la solicitud fue exitosa\n",
    "        print(\"Solicitud a la API exitosa.\")  # Confirmación de que la solicitud fue exitosa\n",
    "\n",
    "        # Obtener los datos en formato JSON\n",
    "        data = response.json()\n",
    "\n",
    "        # Verificar si la respuesta contiene artículos\n",
    "        if 'articles' in data:\n",
    "            articles = data['articles']\n",
    "            print(f\"Se han encontrado {len(articles)} artículos.\")  # Muestra la cantidad de artículos encontrados\n",
    "\n",
    "            # Crear una lista de URLs existentes (para evitar duplicados)\n",
    "            existing_urls = set()\n",
    "\n",
    "            # Filtrar artículos y agregar solo los nuevos\n",
    "            filtered_articles = []\n",
    "            for article in articles:\n",
    "                # Obtener fecha de publicación y convertirla\n",
    "                fecha_publicacion = parser.parse(article['publishedAt'])\n",
    "\n",
    "                # Convertir la fecha de publicación a naive si es aware\n",
    "                fecha_publicacion = fecha_publicacion.replace(tzinfo=None)\n",
    "\n",
    "                # Si no es duplicada, agregarla\n",
    "                if article['url'] not in existing_urls:\n",
    "                    filtered_articles.append({\n",
    "                        'Título': article.get('title', 'Sin título'),\n",
    "                        'Descripción': article.get('description', 'Sin descripción'),\n",
    "                        'Autor': article.get('author', 'Autor no disponible'),\n",
    "                        'URL': article.get('url', 'URL no disponible'),\n",
    "                        'Fecha de publicación': article.get('publishedAt', 'Fecha no disponible'),\n",
    "                        'Imagen': article.get('urlToImage', 'Imagen no disponible')\n",
    "                    })\n",
    "                    existing_urls.add(article['url'])  # Añadir URL a la lista de existentes\n",
    "\n",
    "            # Si se encontraron artículos, convertirlos a DataFrame\n",
    "            if filtered_articles:\n",
    "                df_api_1 = pd.DataFrame(filtered_articles)\n",
    "\n",
    "                # Mostrar los primeros registros del DataFrame\n",
    "                print(\"Artículos filtrados y convertidos a DataFrame:\")\n",
    "                print(df_api_1.head())  # Muestra las primeras filas del DataFrame\n",
    "\n",
    "                # Guardar el DataFrame en un archivo CSV\n",
    "                df_api_1.to_csv('api_newsapi', index=False)\n",
    "                print('Datos guardados en \"api_newsapi\".')  # Mensaje cuando los datos son guardados\n",
    "            else:\n",
    "                print('No se encontraron artículos recientes de tecnología.')\n",
    "\n",
    "        else:\n",
    "            print('No se encontraron artículos en la respuesta de la API.')\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error al hacer la solicitud a la API: {e}')\n",
    "\n",
    "\n",
    "# Llamar a la función para obtener noticias de inmediato\n",
    "fetch_news()\n",
    "\n",
    "# Programar la ejecución cada 7 días\n",
    "schedule.every(7).days.do(fetch_news)\n",
    "\n",
    "# Mantener el script en ejecución\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4d412-c3c2-4549-9938-7a7039b06d94",
   "metadata": {},
   "source": [
    "## API MEDIASTACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8dce4fd-4f95-4e28-8c6c-5cb85ae90994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la extracción de noticias...\n",
      "Se han encontrado 25 artículos.\n",
      "Artículos extraídos y convertidos a DataFrame:\n",
      "               author                                              title  \\\n",
      "0  Cheyenne MacDonald  Apple is reportedly trying to make a giant iPa...   \n",
      "1          Simon Hill  6 Best Video Doorbell Cameras (2024): Smart, B...   \n",
      "2      Natasha Singer  How Student Phones and Social Media Are Fuelin...   \n",
      "3    Jordan Michelman   5 Best Carbon Steel Pans For Every Budget (2024)   \n",
      "4                None  'Meu coração dava uns trancos, e um aparelho c...   \n",
      "\n",
      "                                         description  \\\n",
      "0  We’ve been hearing rumors about the foldables ...   \n",
      "1  Never miss a delivery. These WIRED-tested pick...   \n",
      "2  Cafeteria melees. Students kicked in the head....   \n",
      "3  Our expert cooked with 20 pans and these are t...   \n",
      "4  'Meu coração dava uns trancos, e um aparelho c...   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://www.engadget.com/mobile/tablets/apple-...   \n",
      "1  https://www.wired.com/gallery/best-video-doorb...   \n",
      "2  https://www.nytimes.com/2024/12/15/technology/...   \n",
      "3  https://www.wired.com/gallery/the-best-carbon-...   \n",
      "4  https://news.google.com/rss/articles/CBMizgFBV...   \n",
      "\n",
      "                      source  \\\n",
      "0                   Engadget   \n",
      "1                      Wired   \n",
      "2         The New York Times   \n",
      "3                      Wired   \n",
      "4  Google News Technology BR   \n",
      "\n",
      "                                               image    category language  \\\n",
      "0  https://o.aolcdn.com/images/dims?image_uri=htt...  technology       en   \n",
      "1  https://media.wired.com/photos/6648e7def19d97b...  technology       en   \n",
      "2  https://static01.nyt.com/images/2024/11/25/mul...  technology       en   \n",
      "3  https://media.wired.com/photos/675e4eeefcd82ef...  technology       en   \n",
      "4                                               None  technology       pt   \n",
      "\n",
      "  country               published_at  \n",
      "0      us  2024-12-15T16:08:21+00:00  \n",
      "1      us  2024-12-15T14:02:00+00:00  \n",
      "2      us  2024-12-15T13:15:14+00:00  \n",
      "3      us  2024-12-15T13:01:00+00:00  \n",
      "4      br  2024-12-15T10:01:50+00:00  \n",
      "Datos guardados en \"articulos_technology.csv\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Programar la ejecución cada 12 horas\\nschedule.every(12).hours.do(fetch_news)\\n\\n# Mantener el script en ejecución\\nwhile True:\\n    schedule.run_pending()\\n    time.sleep(1)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "# Función para obtener noticias de la API\n",
    "def fetch_news():\n",
    "    print(\"Iniciando la extracción de noticias...\")  # Mensaje para indicar que la función ha comenzado\n",
    "\n",
    "    # Definir la URL de la API con parámetros de categorías\n",
    "    url = 'https://api.mediastack.com/v1/news'\n",
    "    params = {\n",
    "        'access_key': 'dff24c36aa79823c1effa0ff3a93f11a',  # Reemplaza con tu clave de acceso\n",
    "        'categories': 'technology',  # Solo noticias sobre tecnología\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Realizar la solicitud a la API\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        # Verificar si la solicitud fue exitosa\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Obtener los datos en formato JSON\n",
    "        data = response.json()\n",
    "        \n",
    "        # Verificar si la respuesta contiene artículos\n",
    "        if 'data' in data:\n",
    "            articles = data['data']\n",
    "            print(f\"Se han encontrado {len(articles)} artículos.\")  # Muestra la cantidad de artículos encontrados\n",
    "            \n",
    "            # Convertir los artículos a un DataFrame\n",
    "            df_api_2 = pd.DataFrame(articles)\n",
    "            \n",
    "            # Mostrar el DataFrame resultante\n",
    "            print(\"Artículos extraídos y convertidos a DataFrame:\")\n",
    "            print(df_api_2.head())  # Muestra las primeras filas del DataFrame\n",
    "            \n",
    "            # Guardar el DataFrame en un archivo CSV\n",
    "            df_api_2.to_csv('api_mediastack.csv', index=False)\n",
    "            print('Datos guardados en \"api_mediastack\".')  # Mensaje cuando los datos son guardados\n",
    "        else:\n",
    "            print('No se encontraron artículos en la respuesta.')\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error al hacer la solicitud a la API: {e}')\n",
    "\n",
    "\n",
    "# Llamar a la función para obtener noticias de inmediato\n",
    "fetch_news()\n",
    "\n",
    "# Programar la ejecución cada 7 días\n",
    "schedule.every(7).days.do(fetch_news)\n",
    "\n",
    "# Mantener el script en ejecución\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76928c67-ac2f-44ab-a383-395365363712",
   "metadata": {},
   "source": [
    "## API CURRENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8f1f2d-4923-47ba-bc3c-b54edddf7ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la extracción de noticias...\n",
      "Se han encontrado 30 artículos.\n",
      "Artículos extraídos y convertidos a DataFrame:\n",
      "                                     id  \\\n",
      "0  b848a6fd-bcea-4fe6-91c4-8242c98ff413   \n",
      "1  edbbd976-3e77-488f-8366-bc660e92f9fd   \n",
      "2  2ed06a18-9ff5-40a4-8287-e223a15c4f94   \n",
      "3  ba97c910-e870-4c87-854e-d298fa4862e4   \n",
      "4  78f29e2f-7731-40b2-a392-fa2ef86f0d40   \n",
      "\n",
      "                                               title  \\\n",
      "0  The PlayStation 5 laptop is a 9.5 pound portab...   \n",
      "1  Why the U.S. government is saying all citizens...   \n",
      "2   Intel Core Ultra 9 285K vs. Intel Core i9 14900K   \n",
      "3  Xfce 4.20 Desktop Released With Wayland Improv...   \n",
      "4  Roguelike ARPG Wizard of Legend 2 adds new bio...   \n",
      "\n",
      "                                         description  \\\n",
      "0  Weighing in at a whopping 9.5 pounds for the l...   \n",
      "1  Think twice before sending your next text mess...   \n",
      "2  Intel's two latest flagships, the Core Ultra 9...   \n",
      "3  After roughly two years of development the Xfc...   \n",
      "4  As Wizard of Legend 2 early access rolls on, d...   \n",
      "\n",
      "                                                 url               author  \\\n",
      "0  https://www.techspot.com/news/105966-playstati...             Zo Ahmed   \n",
      "1  https://www.cnbc.com/2024/12/15/why-the-fbi-wa...  Cheryl Winokur Munk   \n",
      "2  https://www.techspot.com/review/2934-intel-cor...        Steven Walton   \n",
      "3   https://www.phoronix.com/news/Xfce-4.20-Released      Michael Larabel   \n",
      "4  https://www.pcgamesn.com/wizard-of-legend-2/up...           Ken Allsop   \n",
      "\n",
      "                                               image language  \\\n",
      "0  https://www.techspot.com/images2/news/bigimage...       en   \n",
      "1  https://image.cnbcfm.com/api/v1/image/10807538...       en   \n",
      "2  https://www.techspot.com/articles-info/2934/im...       hy   \n",
      "3                                               None       en   \n",
      "4                                               None       en   \n",
      "\n",
      "                    category                  published  \n",
      "0               [technology]  2024-12-15 15:32:00 +0000  \n",
      "1               [technology]  2024-12-15 14:30:02 +0000  \n",
      "2               [technology]  2024-12-15 12:49:00 +0000  \n",
      "3  [technology, programming]  2024-12-15 11:34:32 +0000  \n",
      "4         [game, technology]  2024-12-15 11:24:15 +0000  \n",
      "Datos guardados en \"articulos_technology_currents.csv\".\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "# Función para obtener noticias de la API de Currents\n",
    "def fetch_news():\n",
    "    print(\"Iniciando la extracción de noticias...\")  # Mensaje para indicar que la función ha comenzado\n",
    "\n",
    "    # Definir la URL de la API de Currents\n",
    "    url = 'https://api.currentsapi.services/v1/search'\n",
    "    params = {\n",
    "        'apiKey': 'nIHmXWdFkHq6lDWzuvQFkABoBTTbScxztWpYiII_dCiuWLQ9',  # Reemplaza con tu clave de API\n",
    "        'category': 'technology',  # Solo noticias sobre tecnología\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Realizar la solicitud a la API\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        # Verificar si la solicitud fue exitosa\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Obtener los datos en formato JSON\n",
    "        data = response.json()\n",
    "        \n",
    "        # Verificar si la respuesta contiene artículos\n",
    "        if 'news' in data:\n",
    "            articles = data['news']\n",
    "            print(f\"Se han encontrado {len(articles)} artículos.\")  # Muestra la cantidad de artículos encontrados\n",
    "            \n",
    "            # Convertir los artículos a un DataFrame\n",
    "            df_api_3 = pd.DataFrame(articles)\n",
    "            \n",
    "            # Mostrar el DataFrame resultante\n",
    "            print(\"Artículos extraídos y convertidos a DataFrame:\")\n",
    "            print(df_api_3.head())  # Muestra las primeras filas del DataFrame\n",
    "            \n",
    "            # Guardar el DataFrame en un archivo CSV\n",
    "            df_api_3.to_csv('api_currents.csv', index=False)\n",
    "            print('Datos guardados en \"api_currents.csv\".')  # Mensaje cuando los datos son guardados\n",
    "        else:\n",
    "            print('No se encontraron artículos en la respuesta.')\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error al hacer la solicitud a la API: {e}')\n",
    "\n",
    "fetch_news()\n",
    "\n",
    "# Programar la ejecución cada 7 días\n",
    "schedule.every(7).days.do(fetch_news)\n",
    "\n",
    "# Mantener el script en ejecución\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b36e969-686e-4d3a-b65e-ddfc7756deea",
   "metadata": {},
   "source": [
    "## WEBSCRAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb61c38-86fc-44e8-8982-b9a3b60f4384",
   "metadata": {},
   "source": [
    "#### - HACKER NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1802caa5-050b-4f00-ae83-1ae4530ee2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instalando requests...\n",
      "Instalando beautifulsoup4...\n",
      "¡Todas las bibliotecas se instalaron correctamente!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_libraries():\n",
    "    try:\n",
    "        # Lista de bibliotecas a instalar\n",
    "        libraries = [\"requests\", \"beautifulsoup4\"]\n",
    "        \n",
    "        for library in libraries:\n",
    "            print(f\"Instalando {library}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", library])\n",
    "        print(\"¡Todas las bibliotecas se instalaron correctamente!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante la instalación: {e}\")\n",
    "\n",
    "# Llama a la función para instalar las bibliotecas\n",
    "install_libraries()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daef394f-81d5-48a8-8b5f-1f6882431971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El paquete 'newspaper3k' se instaló correctamente.\n",
      "El paquete 'lxml[html_clean]' se instaló correctamente.\n"
     ]
    }
   ],
   "source": [
    "import subprocess  # Ejecutar comandos del sistema\n",
    "import sys  # Obtener la ruta del intérprete de Python\n",
    "\n",
    "def install_packages(packages):\n",
    "    \"\"\"\n",
    "    Instala los paquetes proporcionados usando pip.\n",
    "    \"\"\"\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"El paquete '{package}' se instaló correctamente.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error al instalar '{package}': {e}\")\n",
    "\n",
    "# Instalar Newspaper3k y lxml[html_clean]\n",
    "install_packages([\"newspaper3k\", \"lxml[html_clean]\"])\n",
    "\n",
    "\"\"\"\n",
    "- 'newspaper3k': Librería para extraer noticias de sitios web. Facilita el scraping de artículos y metadatos.\n",
    "- 'lxml[html_clean]': Librería para procesar HTML, limpiando etiquetas innecesarias durante el scraping.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d368a314-914c-4ff4-b096-a7c0988024be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping página 1: https://news.ycombinator.com\n",
      "Error al extraer descripción del artículo: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.pcmag.com/articles/nyc-wants-you-to-stop-taking-traffic-cam-selfies-but-heres-how-to-do-it on URL https://www.pcmag.com/articles/nyc-wants-you-to-stop-taking-traffic-cam-selfies-but-heres-how-to-do-it\n",
      "Error al extraer descripción del artículo: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/pii/S2666202724004518 on URL https://www.sciencedirect.com/science/article/pii/S2666202724004518\n",
      "Error al extraer descripción del artículo: Article `download()` failed with HTTPSConnectionPool(host='news.ycombinator.comitem', port=443): Max retries exceeded with url: /?id=42418157 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027DB99F8500>: Failed to resolve 'news.ycombinator.comitem' ([Errno 11001] getaddrinfo failed)\")) on URL https://news.ycombinator.comitem?id=42418157\n",
      "Scraping página 2: https://news.ycombinator.com?p=2\n",
      "Error al extraer descripción del artículo: Article `download()` failed with HTTPSConnectionPool(host='svs.gsfc.nasa.gov', port=443): Max retries exceeded with url: /4850 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000027DB9AA6E70>, 'Connection to svs.gsfc.nasa.gov timed out. (connect timeout=7)')) on URL https://svs.gsfc.nasa.gov/4850\n",
      "Error al extraer descripción del artículo: Article `download()` failed with HTTPSConnectionPool(host='news.ycombinator.comitem', port=443): Max retries exceeded with url: /?id=42421052 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027DB9AF08C0>: Failed to resolve 'news.ycombinator.comitem' ([Errno 11001] getaddrinfo failed)\")) on URL https://news.ycombinator.comitem?id=42421052\n",
      "Error al extraer descripción del artículo: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.wsj.com/lifestyle/careers/remote-work-in-office-return-fired-f81b1c3c on URL https://www.wsj.com/lifestyle/careers/remote-work-in-office-return-fired-f81b1c3c\n",
      "Error al extraer descripción del artículo: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.cell.com/cell/abstract/S0092-8674(24)00897-3 on URL https://www.cell.com/cell/abstract/S0092-8674(24)00897-3\n",
      "Scraping página 3: https://news.ycombinator.com?p=3\n",
      "Error al extraer descripción del artículo: Article `download()` failed with HTTPSConnectionPool(host='news.ycombinator.comitem', port=443): Max retries exceeded with url: /?id=42410582 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027DBAC97020>: Failed to resolve 'news.ycombinator.comitem' ([Errno 11001] getaddrinfo failed)\")) on URL https://news.ycombinator.comitem?id=42410582\n",
      "Error al extraer descripción del artículo: Article `download()` failed with 404 Client Error: Not Found for url: https://www.defense.gov/News/News-Stories/Article/Article/4000004/test/ on URL https://www.defense.gov/News/News-Stories/Article/Article/4000004/test/\n",
      "Error al extraer descripción del artículo: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.science.org/content/article/amid-cuts-basic-research-new-zealand-scraps-all-support-social-sciences on URL https://www.science.org/content/article/amid-cuts-basic-research-new-zealand-scraps-all-support-social-sciences\n",
      "Error al extraer descripción del artículo: Article `download()` failed with 403 Client Error: Forbidden for url: https://openai.com/index/elon-musk-wanted-an-openai-for-profit/ on URL https://openai.com/index/elon-musk-wanted-an-openai-for-profit/\n",
      "                                               Title  \\\n",
      "0  School smartphone ban results in better sleep ...   \n",
      "1  Tenstorrent and the State of AI Hardware Startups   \n",
      "2  IRATA.ONLINE: A Community for Retro-Computing ...   \n",
      "3  Man ran 700 miles to make 'insanely impressive...   \n",
      "4  Should programming languages be safe or powerful?   \n",
      "\n",
      "                                                Link  \\\n",
      "0  https://www.york.ac.uk/news-and-events/news/20...   \n",
      "1  https://irrationalanalysis.substack.com/p/tens...   \n",
      "2                              https://irata.online/   \n",
      "3  https://www.washingtonpost.com/lifestyle/2024/...   \n",
      "4  https://lambdaland.org/posts/2024-11-21_powerf...   \n",
      "\n",
      "                                         Description     Author  \n",
      "0  School smartphone ban results in better sleep ...   jonatron  \n",
      "1  Made a framework for thinking about AI hardwar...        zdw  \n",
      "2  ABOUTFun, Social, Creative, Retro.\\nIRATA.ONLI...  Bluestein  \n",
      "3  He made it using the popular GPS-tracking app ...  bookofjoe  \n",
      "4  21 Nov 2024Should a programming language be po...   soegaard  \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  # Librería para parsear y extraer información de documentos HTML\n",
    "import requests  # Librería para hacer solicitudes HTTP\n",
    "from newspaper import Article  # Librería para extraer artículos de noticias de URLs\n",
    "import pandas as pd  # Librería para manejar y procesar datos en forma de tablas (DataFrames)\n",
    "\n",
    "def scrape_hacker_news(pages=1):\n",
    "    \"\"\"\n",
    "    Función para hacer scraping de noticias de Hacker News (https://news.ycombinator.com)\n",
    "    \n",
    "    Argumentos:\n",
    "    pages -- Número de páginas a scrapear (por defecto 1).\n",
    "    \n",
    "    Retorna:\n",
    "    all_news -- Lista con los títulos, enlaces, descripciones y autores de las noticias extraídas.\n",
    "    \"\"\"\n",
    "    base_url = \"https://news.ycombinator.com\"  # URL base de Hacker News\n",
    "    url = base_url  # Inicializamos la URL a la base\n",
    "    all_news = []  # Lista para almacenar las noticias extraídas\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "    }  # Encabezados HTTP para simular una solicitud de navegador\n",
    "\n",
    "    # Iterar sobre las páginas que se desean scrapear\n",
    "    for page in range(pages):\n",
    "        print(f\"Scraping página {page + 1}: {url}\")\n",
    "        response = requests.get(url, headers=headers)  # Realizar la solicitud HTTP\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error al acceder a la página: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")  # Parsear el HTML de la página\n",
    "\n",
    "        # Buscar los elementos que contienen las noticias\n",
    "        news_items = soup.find_all(\"span\", class_=\"titleline\")\n",
    "\n",
    "        if not news_items:\n",
    "            print(\"No se encontraron noticias en esta página.\")\n",
    "            break\n",
    "\n",
    "        # Iterar sobre cada noticia encontrada\n",
    "        for item in news_items:\n",
    "            link_tag = item.find(\"a\")  # Buscar el enlace dentro del título\n",
    "            if link_tag:\n",
    "                title = link_tag.text.strip()  # Obtener el título de la noticia\n",
    "                link = link_tag[\"href\"]  # Obtener el enlace de la noticia\n",
    "                if not link.startswith(\"http\"):\n",
    "                    link = base_url + link  # Convertir enlaces relativos a absolutos\n",
    "\n",
    "                # Intentar generar la descripción del artículo usando 'newspaper'\n",
    "                description = \"No description available\"\n",
    "                try:\n",
    "                    article = Article(link)  # Crear un objeto Article con el enlace\n",
    "                    article.download()  # Descargar el contenido del artículo\n",
    "                    article.parse()  # Parsear el contenido\n",
    "                    article.nlp()  # Realizar análisis de texto (por ejemplo, resumen)\n",
    "                    description = article.summary  # Obtener el resumen del artículo\n",
    "                except Exception as e:\n",
    "                    print(f\"Error al extraer descripción del artículo: {e}\")\n",
    "\n",
    "                # Buscar el autor de la noticia\n",
    "                parent_tr = item.find_parent(\"tr\")  # Obtener la fila que contiene la noticia\n",
    "                subline_tr = parent_tr.find_next_sibling(\"tr\")  # Obtener la siguiente fila con detalles\n",
    "                author_tag = subline_tr.find(\"a\", class_=\"hnuser\")  # Buscar el autor dentro del <a>\n",
    "                author = author_tag.text if author_tag else \"No Author\"  # Obtener el nombre del autor\n",
    "\n",
    "                all_news.append((title, link, description, author))  # Agregar la noticia a la lista\n",
    "\n",
    "        # Buscar el enlace para la siguiente página\n",
    "        more_link = soup.find(\"a\", class_=\"morelink\")\n",
    "        if more_link:\n",
    "            url = base_url + more_link[\"href\"]  # Si hay una página siguiente, actualizar la URL\n",
    "        else:\n",
    "            print(\"No se encontró el enlace 'More'. Terminando scraping.\")\n",
    "            break\n",
    "\n",
    "    return all_news  # Retornar la lista de todas las noticias extraídas\n",
    "\n",
    "# Extraer noticias de las primeras 3 páginas\n",
    "news = scrape_hacker_news(pages=3)\n",
    "\n",
    "# Convertir los resultados a un DataFrame para su procesamiento\n",
    "df = pd.DataFrame(news, columns=[\"Title\", \"Link\", \"Description\", \"Author\"])\n",
    "\n",
    "# Limpiar el DataFrame eliminando las filas sin descripción\n",
    "df_clean = df[df['Description'] != \"No description available\"]\n",
    "\n",
    "# Mostrar las primeras noticias, si es que existen\n",
    "if not df_clean.empty:\n",
    "    print(df_clean.head())  # Mostrar las primeras 5 noticias\n",
    "else:\n",
    "    print(\"No se encontraron noticias con descripción.\")  # Si no hay noticias con descripción\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ddc8b5-ac2e-4c7a-9096-ae80ccecec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame en un archivo CSV\n",
    "df_clean.to_csv('webscraping_hackernews.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
