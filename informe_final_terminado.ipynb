{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b950a9-643d-467d-9088-8e69662c93b0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h1>UNIVERSIDAD NACIONAL AGRARIA LA MOLINA</h1>\n",
    "    <h2>DEPARTAMENTO ACADÉMICO DE ESTADÍSTICA INFORMÁTICA</h2>\n",
    "    <img src=\"https://www.lamolina.edu.pe/portada/html/acerca/escudos/download/color/856x973_ESCUDOCOLOR.jpg\" alt=\"Escudo de la Universidad\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "**Curso:** Lenguaje de Programación II\n",
    "\n",
    "**Tema:** El seguimiento de noticias tecnológicas mediante el uso de API's y web scraping\n",
    "\n",
    "**Integrantes:**\n",
    "\n",
    "| **Integrantes**                           | **Código**   |\n",
    "|-------------------------------------------|--------------|\n",
    "| Paredes Ballenas, Brisa Cielo             | 20221411     |\n",
    "| Santillan Tsejem, Sergio                  | 20191314     |\n",
    "| Sencara Maldonado, Jorge Abel             | 20231508     |\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introducción](#introducción)  \n",
    "2. [Objetivos](#objetivos)  \n",
    "3. [Metodología](#metodología)  \n",
    "   - [Planificación](#planificación)  \n",
    "   - [Extracción de Datos](#extracción-de-datos)  \n",
    "     - [Fuentes de Datos](#fuentes-de-datos)  \n",
    "     - [Herramientas](#herramientas)  \n",
    "     - [Procesos](#procesos)  \n",
    "   - [Análisis de Datos](#análisis-de-datos)  \n",
    "     - [Limpieza de Datos](#limpieza-de-datos)  \n",
    "     - [Imputación y Tratamiento de Datos](#imputación-y-tratamiento-de-datos)  \n",
    "     - [Análisis Exploratorio](#análisis-exploratorio)\n",
    "     - [Procesos](#procesos)  \n",
    "   - [Visualización de Datos](#visualización-de-datos)  \n",
    "   - [Integración de Códigos](#integración-de-códigos)  \n",
    "4. [Plataforma Desarrollada](#plataforma-desarrollada)  \n",
    "5. [Resultados](#resultados)  \n",
    "6. [Conclusiones](#conclusiones)  \n",
    "   - [Lecciones Aprendidas](#lecciones-aprendidas)  \n",
    "   - [Impacto del Proyecto](#impacto-del-proyecto)  \n",
    "\n",
    "---\n",
    "\n",
    "## Introducción\n",
    "<a id=\"introducción\"></a>\n",
    "\n",
    "El siguiente proyecto busca facilitar la búsqueda de noticias relevantes, acorde a los intereses del usuario, en torno al tema de la tecnología y temas similares. Para lograr esto, hicimos uso de conocimientos adquiridos durante el curso de Lenguaje de Programación I y II, como la extracción de datos por WebScrapping y API's donde agrupamos noticias tecnológicas de diferentes fuentes, análisis de los datos recolectados, creación de gráficos con librerías en python y al mismo tiempo aprendimos a crear una plataforma con la ayuda da la librería Streamlit, la cual facilitará la visualización final de nuestro proyecto y permitirá seleccionar las noticias con palabras clave y tendencias.\n",
    "\n",
    "Además de esto, deseamos reflejar la importancia de la colaboración en este proyecto, haciendo énfasis las habilidades más importantes para lograr esto, como la constante comunicación de los integrantes del grupo, planificación de reuniones, uso de habilidades blandas, resolución de problemas y la integración todos los aportes para la creación del producto final.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos\n",
    "<a id=\"Objetivos\"></a>\n",
    "El objetivo principal de este proyecto es desarrollar una plataforma personalizada que permita a los usuarios acceder a noticias relacionadas con áreas específicas de la tecnología, como inteligencia artificial, estilo de vida, ciberseguridad, gaming, políticas, automóviles, entre otras.\n",
    "\n",
    "Para lograr esto, se integrarán diversas APIs y técnicas de web scraping con los siguientes objetivos:\n",
    "\n",
    "1. Recopilar noticias de manera eficiente desde fuentes confiables, asegurando información valiosa y relevante para los usuarios.\n",
    "2. Filtrar y organizar la información por categorías tecnológicas, permitiendo al mismo tiempo cumplir con las necesidades de los usuarios mediante una función que permita obtener artículos basados en palabras clave específicas.\n",
    "3. Ofrecer una interfaz clara y accesible para que los usuarios puedan explorar las noticias de forma intuitiva, complementada con gráficos interactivos que proporcionen información adicional sobre la plataforma.\n",
    "4. Fomentar un buen desarrollo tanto grupal como individual, consolidando y demostrando los conocimientos adquiridos en el curso de Lenguaje de Programación I y II.\n",
    "\n",
    "---\n",
    "\n",
    "## Metodología\n",
    "<a id=\"metodología\"></a>\n",
    "\n",
    "Esta plataforma busca ser una herramienta versátil para personas interesadas en el sector tecnológico, ofreciendo un acceso sencillo para poder realizar búsquedas de noticias en la base de datos que poseemos, producto de la extracción previa de los mismos. Para ello, hemos estructurado el proyecto en las siguientes partes:\n",
    "\n",
    "#### Planificación\n",
    "<a id=\"Planificación\"></a>\n",
    "\n",
    "1. **Elección del tema del proyecto final**\n",
    "Al buscar entre diferentes opciones, nos pareció interesante poder realizar nuestro propia plataforma de búsqueda y análisis de noticias, la cual tiene como función facilitar el acceso a no solamente una fuente, de forma que el usuario pueda tener una visión más general de cómo se encuentran las tendencias actualmente.\n",
    "2. **Puntos importantes del proceso de creación de la plataforma**\n",
    "- Extracción de datos por WebScrapping y API's\n",
    "- Análisis de los datos extraídos\n",
    "- Visualización de gráficos con la data analizada\n",
    "- Creación plataforma con Streamlit\n",
    "- Documentación en Jupyter Notebook\n",
    "- Uso de Github y Visual Code Studio para publicación del trabajo final\n",
    "\n",
    "#### Extracción de Datos\n",
    "1. **Fuentes de Datos:**\n",
    "Lo primero fue realizar la búsqueda de fuentes para la extracción de datos, donde las primera opciones fueron las grandes páginas de noticias. Sin embargo, se identificó el primer obstáculo del proyecto: los permisos para usar api's o realizar webscraping. Descubrimos que la mayoria de páginas de noticias más conocidas cuenta con métodos de pago para poder acceder a la base de datos de cada página (API's), y que muchas tenían bloqueos para realizar webscraping.\n",
    "   - Frente a esta problemática, la solución fue poder buscar fuentes que se presten para webscraping y extración por APIS's de forma más sencilla, accesible y sin costos. Las fuentes que usaremos en eel siguiente proyecto fueron:\n",
    "   - Extracción por APIs:\n",
    "     - **NewsAPI:** Proporciona noticias actualizadas de múltiples fuentes globales.  \n",
    "     - **MediaStack:** API con acceso a noticias en tiempo real y estructuradas.  \n",
    "     - **Currents API:** Permite filtrar noticias por temas específicos y fechas.  \n",
    "   - Extracción por Web Scraping: \n",
    "     - **Hacker News:** Fuente de noticias tecnológicas y tendencias sobre startups, accesible mediante scraping. \n",
    "2. **Herramientas:**\n",
    "   - `requests`: Para realizar solicitudes HTTP y obtener datos de las APIs.  \n",
    "   - `pandas`: Para manipular, organizar y procesar los datos obtenidos.  \n",
    "   - `BeautifulSoup`: Para realizar scraping de páginas HTML y extraer información.  \n",
    "   - `Selenium`: Para automatizar la navegación web y extracción de datos dinámicos.  \n",
    "   - `json`: Para interpretar y trabajar con respuestas en formato JSON.  \n",
    "   - Jupyter Notebook  \n",
    "   - GitHub\n",
    "\n",
    "3. **Procesos**:\n",
    "- **Investigación de Fuentes de Datos**:\n",
    "  - Se evaluaron las APIs mencionadas y se estudió su documentación oficial para entender su funcionamiento y limitaciones.\n",
    "- **Extracción mediante APIs**:\n",
    "  - Se implementaron scripts usando la librería `requests` para realizar solicitudes HTTP a las APIs.  \n",
    "   - Se configuraron parámetros como:  \n",
    "     - **Categorías** (tecnología)  \n",
    "     - **Palabras clave** (.)  \n",
    "     - **Fechas** (noticias recientes).   \n",
    "#### Análisis de Datos\n",
    "1. **Limpieza de datos:**\n",
    "\n",
    "- **Revisión Inicial de los Archivos:**  \n",
    "Se realizó una revisión inicial de cada archivo para entender su estructura y contenido. Esto incluyó verificar el número de filas y columnas, los nombres de las columnas y el tipo de datos presente. El objetivo era identificar posibles problemas, como valores faltantes, inconsistencias o columnas innecesarias.\n",
    "\n",
    "- **Eliminación de Columnas Innecesarias:**  \n",
    "Se eliminaron aquellas columnas que no aportaban valor al análisis y que podrían generar ruido en los datos. Esto se realizó basándose en el contenido y la relevancia de cada columna. Se mantuvieron solo las columnas necesarias para el análisis, garantizando que el DataFrame tuviera información limpia y procesable.\n",
    "\n",
    "- **Mejora de Nombres de Columnas:**  \n",
    "Se corrigieron y estandarizaron los nombres de las columnas para eliminar espacios, caracteres especiales y otras inconsistencias. Esto facilitó la manipulación y el análisis posterior, asegurando nombres más descriptivos y fáciles de referenciar.\n",
    "\n",
    "- **Juntar Columnas de las 4 APIs:**  \n",
    "Se combinaron las columnas relevantes de las cuatro APIs (Currents, Mediastack, NewsAPI, HackerNews) para crear una estructura consolidada con las siguientes columnas: `title`, `description`, `url` y `author`. Esto permitió tener un DataFrame único y uniforme para realizar el análisis y comparar información de fuentes distintas.\n",
    "\n",
    "- **Juntar CSVs con Columnas Ordenadas en Común de las 4 APIs:**  \n",
    "Se unificaron archivos CSV provenientes de las cuatro APIs, garantizando que las columnas estuvieran ordenadas de manera consistente. Esto se hizo para crear un DataFrame cohesivo donde las columnas tuvieran el mismo orden y nombres, facilitando la integración y comparación entre fuentes sin errores ni desajustes.\n",
    "\n",
    "- **Colocar Etiquetas para Identificar su Fuente de Origen:**  \n",
    "Se añadieron etiquetas en cada fila para identificar la fuente original de cada enlace o contenido. Estas etiquetas incluyeron el nombre de la API o el dominio de la fuente (por ejemplo, `Currents`, `NewsAPI`). Esto permitió rastrear y verificar de dónde proviene cada información, aportando transparencia y contexto crucial para el análisis y la validación de datos.\n",
    "\n",
    "2. **Análisis Exploratorio:**  \n",
    "El análisis exploratorio tiene como objetivo comprender las características principales del conjunto de datos y descubrir patrones, tendencias o anomalías. Se realizaron las siguientes acciones:  \n",
    "\n",
    "- **Revisión General de los Datos**  \n",
    "   Se observó la estructura del conjunto de datos, las columnas disponibles y el contenido de las filas. Esto permitió identificar variables clave, como los títulos de las noticias y sus respectivas fuentes.  \n",
    "\n",
    "- **Propósito**  \n",
    "   - Detectar tendencias iniciales en los datos.  \n",
    "   - Identificar posibles inconsistencias o datos atípicos.  \n",
    "   - Preparar los datos para futuras etapas de análisis o visualización.  \n",
    "\n",
    "- **Observaciones Preliminares**  \n",
    "   En esta etapa, se observó que:  \n",
    "   - Los títulos de las noticias contienen información relevante para analizar la temática predominante.  \n",
    "   - Existen múltiples fuentes de origen, lo cual permite evaluar su distribución en términos de frecuencia y relevancia.  \n",
    "\n",
    "El análisis exploratorio sentó las bases para la limpieza y visualización posterior de los datos, facilitando la identificación de patrones más específicos.\n",
    "\n",
    "\n",
    "\n",
    "#### Visualización de Datos  \n",
    "Algunos de nuestros gráficos fueron:\n",
    "\n",
    "- Nube de Palabras:  \n",
    "Se creó una nube de palabras a partir de los títulos de noticias para identificar las palabras más frecuentes. Para esto, se limpiaron los textos de los títulos, eliminando puntuación y stopwords, y se generó una representación visual de las palabras más recurrentes. La limpieza del texto incluye convertir todo a minúsculas, eliminar puntuación y eliminar palabras vacías en inglés.\n",
    "Este gráfico muestra una representación visual de las palabras más relevantes y recurrentes en los títulos de las noticias. La limpieza y procesamiento del texto aseguran que la visualización refleje únicamente contenido significativo y útil.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"attachment:495baded-de3f-4ddd-8972-83d3d6a16048.jpeg\" alt=\"Nube de Palabras\" width=\"600\" />\n",
    "</div>\n",
    "\n",
    "- Análisis de Fuentes:\n",
    "Para entender qué sitios web tienen más presencia en el dataset, se creó un gráfico de barras que muestra las 10 fuentes más frecuentes. Primero, se procesaron y normalizaron las URLs para eliminar el prefijo `www`. Luego, se asignaron nombres unificados basados en un mapeo predefinido de dominios a fuentes.\n",
    "Este gráfico ilustra las 10 fuentes de noticias más representadas en el conjunto de datos. Proporciona una visión clara de qué medios tienen más impacto en el contenido analizado.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"attachment:4fe796db-a014-493f-8ec4-2bc29d755a8a.png\" alt=\"Gráfico de Barras\" width=\"600\" />\n",
    "</div>\n",
    "\n",
    "#### Integración de Códigos\n",
    "Durante el desarrollo de este proyecto, nuestro equipo utilizó GitHub como herramienta principal para gestionar el código, colaborar de manera eficiente y realizar un seguimiento adecuado de los cambios. Esta plataforma nos permitió integrar los distintos componentes del proyecto de forma organizada y mantener una estructura de trabajo colaborativo.\n",
    "\n",
    "- **Uso de GitHub para el Control de Versiones**\n",
    "Para organizar y gestionar el proyecto de manera eficiente, creamos carpetas específicas dentro de nuestro repositorio de GitHub, cada una correspondiente a un módulo del proyecto: Extracción de Datos, Análisis de Datos, Visualización de Datos y Desarrollo de la Plataforma. Esta estructura modular permitió que cada miembro del equipo pudiera trabajar en una parte del proyecto sin interferir con las tareas de los demás. Gracias a esta organización, pudimos abordar diferentes aspectos del proyecto simultáneamente de manera ordenada, facilitando la integración de las funcionalidades.\n",
    "\n",
    "- **Desarrollo de Funcionalidades**\n",
    "A medida que avanzábamos en la implementación de las diversas funcionalidades del proyecto, cada miembro del equipo se encargó de tareas específicas. Por ejemplo, mientras algunos se enfocaban en la extracción y almacenamiento de noticias a través de APIs y técnicas de web scraping, otros trabajaban en la parte del análisis o la visualización de datos mediante gráficos interactivos. A través de GitHub, pudimos integrar nuestras contribuciones de manera fluida, asegurando que cada funcionalidad se integrara correctamente en la plataforma final. \n",
    "\n",
    "- **Desarrollo Continuo y Despliegue**\n",
    "A lo largo del desarrollo del proyecto, adoptamos un enfoque de desarrollo continuo, realizando commits para guardar nuestro progreso y evitar la pérdida de avances importantes. Cada vez que completábamos una funcionalidad o corregíamos un error, realizábamos un commit para registrar esos cambios en el repositorio.\n",
    "\n",
    "## Resultados\n",
    "En este proyecto, los resultados obtenidos reflejan una integración exitosa de las funcionalidades propuestas. La plataforma logró consolidar y presentar noticias tecnológicas de diversas fuentes de manera organizada, permitiendo a los usuarios explorar contenido específico a través de categorías como inteligencia artificial, ciberseguridad, gaming, entre otras. A través de la implementación de técnicas de web scraping y el uso de APIs de fuentes confiables, pudimos asegurar una actualización continua de las noticias.\n",
    "\n",
    "Además, la integración de análisis de sentimientos en los títulos de las noticias y la visualización interactiva a través de gráficos permitió obtener información adicional y relevante sobre las tendencias del sector tecnológico. Esto no solo enriqueció la experiencia del usuario, sino que también permitió un análisis más profundo de los datos disponibles, proporcionando valor añadido en términos de la interpretación de los mismos.\n",
    "\n",
    "**Indicadores clave:**\n",
    "\n",
    "- *Cobertura de Fuentes:* Se integraron múltiples fuentes confiables, garantizando una cobertura amplia de temas tecnológicos.  \n",
    "\n",
    "- *Interactividad:* Los gráficos generados, como los de la distribución de noticias por categorías y los análisis de sentimientos, mejoraron la accesibilidad y comprensión de los datos.  \n",
    "\n",
    "- *Actualización Dinámica:* La plataforma se actualiza automáticamente, lo que asegura que los usuarios siempre tengan acceso a las últimas noticias.\n",
    "\n",
    "\n",
    "## Conclusiones\n",
    "A lo largo de la implementación del proyecto, el equipo adquirió valiosas lecciones en cuanto a la gestión de datos y la colaboración efectiva. Una de las lecciones más destacadas fue la importancia de trabajar de manera modular y organizada, lo cual fue facilitado por el uso de GitHub para el control de versiones. Este enfoque permitió que, aunque cada miembro se encargara de diferentes funcionalidades, todos pudieran integrar sus partes de manera efectiva.\n",
    "\n",
    "Además, aprendimos sobre la gestión de APIs y cómo adaptar las herramientas disponibles a nuestras necesidades específicas. El proceso de web scraping y análisis de datos también nos brindó una comprensión más profunda de cómo manejar grandes volúmenes de información y extraer valor de fuentes diversas.\n",
    "\n",
    "**Impacto del Proyecto:**\n",
    "\n",
    "Este proyecto no solo cumplió con los objetivos planteados, sino que también proporcionó un impacto positivo tanto en el aprendizaje individual como colectivo. Los miembros del equipo tuvieron la oportunidad de poner en práctica sus conocimientos de programación, análisis de datos y visualización, lo que resultó en una comprensión más sólida de estas áreas.\n",
    "\n",
    "A nivel práctico, la plataforma desarrollada tiene el potencial de ser utilizada por entusiastas de la tecnología para mantenerse informados con las últimas noticias del sector. Además, la integración de herramientas analíticas, como los gráficos interactivos, agrega valor al ofrecer una visión más rica sobre las tendencias y opiniones dentro de las categorías tecnológicas.\n",
    "\n",
    "En resumen, este proyecto sirvió como una experiencia de aprendizaje integral, con un impacto positivo tanto en el desarrollo de habilidades técnicas como en la comprensión del trabajo en equipo en proyectos colaborativos.del Proyecto:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7711428",
   "metadata": {},
   "source": [
    "#### Link de plataforma final\n",
    "https://noticiaslp2.streamlit.app/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
